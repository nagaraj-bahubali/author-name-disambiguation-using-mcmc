{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33937827",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "1.reads the atomic files and pre-process them by removing records with missing \n",
    "  authorId, referenceId, authorName, title, year\n",
    "2.re-index author ids and paper ids\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5393b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pandas import DataFrame\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "gl_author_id = 1\n",
    "gl_reference_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9506eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atomic_file(atomic_name: str, file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a certain atomic file and transforms the content in the form of dataframe\n",
    "\n",
    "    Parameters\n",
    "    atomic_name : Name of the atomic file.\n",
    "    file_path : Path where atomic file resides.\n",
    "\n",
    "    Returns\n",
    "    df : File content in the form of dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path + atomic_name + '.txt',\n",
    "                     sep=\"_|\\||<>|<>|<>|<>\",\n",
    "                     names=['authorId', 'referenceId', 'authorName', 'coauthors', 'title', 'journal', 'year'],\n",
    "                     header=None,\n",
    "                     keep_default_na=True,\n",
    "                     na_values=['None', 'none'],\n",
    "                     on_bad_lines='skip',\n",
    "                     engine=\"python\")\n",
    "    \n",
    "    # Convert to numeric, invalid parsing will be set as NaN\n",
    "    df.authorId = pd.to_numeric(df.authorId, errors='coerce')\n",
    "    df.referenceId = pd.to_numeric(df.referenceId, errors='coerce')\n",
    "    df.year = pd.to_numeric(df.year, errors='coerce')\n",
    "    \n",
    "    # Drop records with missing values\n",
    "    df = df.dropna(subset=['authorId', 'referenceId', 'authorName', 'title', 'year'])\n",
    "    \n",
    "    # Replace NaNs in journal to empty strings\n",
    "    df.journal = df.journal.fillna('')\n",
    "    \n",
    "    df = df.astype({'authorId': np.int32, 'referenceId': np.int32, 'year': np.int32})\n",
    "    \n",
    "    # Lowercase the strings\n",
    "    df = df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21229ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice dataframe into chunks based on author ids\n",
    "def slice_df(df):\n",
    "    author_id_arr = df['authorId'].unique()\n",
    "    df_slices = []\n",
    "    \n",
    "    for idx in author_id_arr:\n",
    "        df_slices.append(df[df.authorId == idx])\n",
    "    \n",
    "    return df_slices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4a723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-index author ids and paper ids\n",
    "def standardise_df(df):\n",
    "    \n",
    "    global gl_author_id \n",
    "    global gl_reference_id \n",
    "    paper_count = len(df)\n",
    "    ref_id_list = [*range(gl_reference_id, gl_reference_id + paper_count, 1)]\n",
    "    df['referenceId'] = ref_id_list\n",
    "    gl_reference_id = gl_reference_id + paper_count\n",
    "    \n",
    "    df['authorId'] = gl_author_id \n",
    "    gl_author_id = gl_author_id + 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2687dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_atomic_file(std_df_list,atomic_name,dump_path):\n",
    "    \n",
    "    file_content = \"\"\n",
    "    for df in std_df_list:\n",
    "        for _, row in df.iterrows():\n",
    "            file_content += str(row['authorId']) + \"_\" + str(row['referenceId']) + \"|\" + str(row['authorName']) + \"<>\" + str(row['coauthors']) + \"<>\" + (str(row['title'])) + \"<>\" + (str(row['journal']) or \"\") + \"<>\" + (str(row['year'])) + \"\\n\" \n",
    "    \n",
    "    path = dump_path + \"and_data/\" \n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    text_file = open(path + str.lower(atomic_name) + \".txt\", \"w\")\n",
    "    text_file.write(file_content)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d27fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_atomic_name_list(dump_path):\n",
    "    and_path = dump_path + \"and_data/\"\n",
    "    meta_path = dump_path + \"meta_data/\"\n",
    "    \n",
    "    os.chdir(and_path)\n",
    "    atomic_names_list = [file[:-4] for file in glob.glob(\"*.txt\") ]\n",
    "    \n",
    "    os.makedirs(meta_path, exist_ok=True)\n",
    "    with open(meta_path + 'atomic_names_list.pickle', 'wb') as handle:\n",
    "        pickle.dump(atomic_names_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f81a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# path to dataset to be pre-processed\n",
    "dataset_file_path = '/Users/nagaraj/Downloads/aminer/'\n",
    "\n",
    "# path to dump pre-processed dataset and new list of atomic names\n",
    "dump_path = '/Users/nagaraj/Downloads/aminer_filtered/'\n",
    "\n",
    "os.chdir(dataset_file_path)\n",
    "atomic_names_list = [file[:-4] for file in glob.glob(\"*.txt\") ]\n",
    "\n",
    "for atomic_name in atomic_names_list:\n",
    "    df = read_atomic_file(atomic_name,dataset_file_path)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(atomic_name,\".txt is skipped due to zero records before/after pre-processing\")\n",
    "        continue\n",
    "\n",
    "    df_slices = slice_df(df)\n",
    "    std_df_list = [standardise_df(df) for df in df_slices]\n",
    "    dump_atomic_file(std_df_list,atomic_name,dump_path)\n",
    "\n",
    "dump_atomic_name_list(dump_path)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a1718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
