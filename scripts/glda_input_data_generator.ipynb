{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdf477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0accf85b",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This script creates all the necessary input data (vocabulary, embeddings of vocabulary and \n",
    "corpus in terms of word ids) for gaussian lda \n",
    "gaussian lda : https://github.com/rajarshd/Gaussian_LDA\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd840a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atomic_file(atomic_name: str, file_path: str):\n",
    "    \"\"\"\n",
    "    Reads a certain atomic file and transforms the content in the form of dataframe\n",
    "\n",
    "    Parameters\n",
    "    atomic_name : Name of the atomic file.\n",
    "    file_path : Path where atomic file resides.\n",
    "\n",
    "    Returns\n",
    "    df : File content in the form of dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path + atomic_name + '.txt',\n",
    "                     sep=\"_|\\||<>|<>|<>|<>\",\n",
    "                     names=['authorId', 'referenceId', 'authorName', 'coauthors', 'title', 'journal', 'year'],\n",
    "                     header=None,\n",
    "                     keep_default_na=False,\n",
    "                     on_bad_lines='skip',\n",
    "                     engine=\"python\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(atomic_list_path,dataset_path):\n",
    "    \n",
    "    with open(atomic_list_path+'atomic_names_list.pickle', 'rb') as handle:\n",
    "        atomic_names_list = pickle.load(handle)\n",
    "    \n",
    "    id_paper_dict = {}\n",
    "    for atomic_name in atomic_names_list:\n",
    "        df = read_atomic_file(atomic_name, dataset_path)\n",
    "        df_id_paper_dict = pd.Series(df.title.values,index=df.referenceId).to_dict()\n",
    "        id_paper_dict.update(df_id_paper_dict)\n",
    "        \n",
    "    return id_paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_list_path= '/Users/nagaraj/Desktop/author-name-disambiguation-using-mcmc/data/input/unified-and-dataset_1_filtered/ethnicity_data/'\n",
    "dataset_path = '/Users/nagaraj/Desktop/author-name-disambiguation-using-mcmc/data/input/unified-and-dataset_1_filtered/and_data/'\n",
    "destination_dump_path = '/Users/nagaraj/Desktop/Gaussian_LDA-master/data/unified-and-dataset_1_filtered/'\n",
    "\n",
    "id_paper_dict = load_data(atomic_list_path,dataset_path)\n",
    "# id_paper_dict = dict(sorted(id_paper_dict.items()))\n",
    "data = list(id_paper_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(str(sentence).split()) \n",
    "        \n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "vocab = [word for word in id2word.values()]\n",
    "embeddings = bert_model.encode(vocab)\n",
    "corpus = [id2word.doc2idx(sent_words) for sent_words in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d65971",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(destination_dump_path + 'vocab.txt', 'w') as file:\n",
    "    vocab_len = len(vocab)\n",
    "    for word,line_num in zip(vocab,range(vocab_len)):\n",
    "        file.write(word)\n",
    "        if line_num < (vocab_len - 1):\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b44f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(destination_dump_path + 'vocab_vectors.txt', embeddings, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(destination_dump_path + 'corpus.txt', 'w') as file:\n",
    "    corpus_len = len(corpus) \n",
    "    for row,line_num in zip(corpus,range(corpus_len)):\n",
    "        file.write(' '.join([str(item) for item in row]))\n",
    "        if line_num < (corpus_len - 1):\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is needed for reconciliation of topic distributions of papers, as the paper ids are not strictly consecutive\n",
    "# eg, 123,124,127\n",
    "paperid2docid = {}\n",
    "docid = 1\n",
    "\n",
    "for _id in id_paper_dict:\n",
    "    paperid2docid[_id] = docid\n",
    "    docid = docid + 1\n",
    "\n",
    "docid2paperid = {v: k for k, v in paperid2docid.items()}\n",
    "\n",
    "with open(destination_dump_path + 'paperid2docid.pickle', 'wb') as handle:\n",
    "    pickle.dump(paperid2docid, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(destination_dump_path + 'docid2paperid.pickle', 'wb') as handle:\n",
    "    pickle.dump(paperid2docid, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcdc00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9dffcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1129fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c532f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to store the topic distributions of papers in the form of paper_id : topic distributions\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "destination_dump_path = '/Users/nagaraj/Desktop/author-name-disambiguation-using-mcmc/data/input/unified-and-dataset_1_filtered/meta_data/'\n",
    "\n",
    "path = '/Users/nagaraj/Desktop/Gaussian_LDA-master/output/unified-and-dataset_1_filtered/'\n",
    "document_topic = np.loadtxt(path + 'document_topic.txt', dtype = np.float128)\n",
    "\n",
    "path = '/Users/nagaraj/Desktop/Gaussian_LDA-master/data/unified-and-dataset_1_filtered/'\n",
    "with open(path + 'paperid2docid.pickle', 'rb') as handle:\n",
    "    paperid2docid = pickle.load(handle)\n",
    "\n",
    "    \n",
    "topic_dist = dict()\n",
    "\n",
    "for paper_id,dist in zip(paperid2docid.keys(),document_topic):\n",
    "    topic_dist[paper_id] = dist\n",
    "    \n",
    "with open(destination_dump_path + 'topic_distributions.pickle', 'wb') as handle:\n",
    "    pickle.dump(topic_dist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1d801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
